[project]
name = "LLM-Scoring-Engine"
version = "0.7.0"
description = "A comprehensive system for evaluating and scoring large language models based on multiple criteria."
dependencies = [
    "typing_extensions>=4.0.0",
    "argparse>=1.4.0",
    "huggingface-hub>=0.20.0",
    "pandas>=2.0.0",
    "plotly",
    "Jinja2",
    "prompt-toolkit>=3.0.0",
    "rich>=13.0.0",
    "typer>=0.9.0",
    "rapidfuzz>=3.0.0",
    "sqlalchemy>=2.0.0",
    "sqlmodel>=0.0.14",
    "pydantic>=2.0.0",
    "pyyaml>=6.0.0",
    "tomlkit>=0.12.0",
    "alembic>=1.12.0",
    "watchdog>=4.0.0",
    "httpx>=0.25.0",
    "croniter>=1.4.0",
    "tenacity>=8.0.0",
    "requests>=2.31.0",
    "tqdm>=4.65.0",
    "python-dotenv>=1.0.0",
    "textual>=0.58.0",
    "aiohttp>=3.9.0"
]
requires-python = ">=3.11"

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[project.optional-dependencies]
dev = [
    "pytest>=7.0.0",
    "pytest-cov>=4.0.0",
    "pytest-benchmark>=5.0.0",
    "pytest-asyncio>=0.23.0",
    "pytest-regressions>=2.0.0",
    "pytest-xdist>=3.0.0",
    "freezegun>=1.0.0",
    "respx>=0.20.0",
    "ruff>=0.1.0",
    "axe-core-python>=0.3.0"
]

[project.scripts]
llmscore = "llmscore.__main__:main"

[tool.hatch.build.targets.wheel]
packages = ["model_scoring", "llmscore"] 

[tool.pytest.ini_options]
markers = [
    "perf: Performance benchmark suite"
]
